<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Occam Shaving (DLRL 2025)</title>
    <style>
        body {
            font-family: sans-serif;
            margin: 40px auto;
            max-width: 650px;
            line-height: 1.6;
            font-size: 18px;
            color: #444;
            padding: 0 10px
        }
        h1, h2, h3, h4, h5, h6 {
            margin-bottom: 0;
            line-height: 1.2;
        }
        .subtitle {
            font-size: 1.25rem;
            margin-top: 0;
            font-weight: bold;
        }
        hr {
            margin: 2em 0;
        }
        code {
            background: #eee;
            color: #275d38;
            padding: 2px 4px;
            margin: -2px 0;
            border: 1px solid #999;
            border-radius: 2px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Occam Shaving <a href="#" style="font-size: 8px;" onclick="javascript:document.body.style.background = 'linear-gradient(green, blue)'; document.body.style.color = 'red'">Enable Speed</a></h1>
        <p class="subtitle">Group Project for the DLRL 2025 Summer School</p>
        By:
        <ul>
            <li>Mikel MalagÃ³n</li>
            <li>Fahd Husain</li>
            <li>Seth Akins</li>
            <li>Mahvash Siavashpour</li>
            <li>Paulius Sasnauskas</li>
            <li>Kunal Samanta</li>
            <li>Bruce Proudfoot</li>
        </ul>
    </header>
    <hr/>
    <main>
        <section>
            <h2>Motivation</h2>
            <time datetime="2025-07-28">July 28</time>
            <p>
                Frustrated by PPO, namely, why does it work, the sensitivity to hyperparameters, trust region value being 0.2 no matter what, and all the implementation details, we set out to figure out if it's possible to make this algorithm simpler.
                We were inspired by <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">The 37 Implementation Details of PPO</a>, and wanted a more principled way of thinking about tricks and hyperparameters.
            </p>
        </section>
        <section>
            <h2>Setup</h2>
            <time datetime="2025-07-28">July 28</time>
            <p>
                After looking through the 37 tricks, we devised our own list, with some high-level groupings of the tricks that made sense to us.
                This resulted in a list of 12 tricks (in 5 categories):
                <ul>
                    <li>Normalization and clipping</li>
                    <ul>
                        <li>Normalization of the advantage estimates (remove? or change to symlog) [ðŸ®²ðŸ®³ Mahvash]</li>
                        <li>Surrogate objective clipping (remove?) [ðŸ®²ðŸ®³ Paulius]</li>
                        <li>Value function clipping (remove?) [ðŸ®²ðŸ®³ Mikel]</li>
                        <li>Gradient clipping (remove?) [ðŸ®²ðŸ®³ Mahvash]</li>
                    </ul>
                    <li>Advantage Estimation</li>
                    <ul>
                        <li>Change GAE to just advantage prediction [ðŸ®²ðŸ®³ Fahd]</li>
                    </ul>
                    <li>Training setup</li>
                    <ul>
                        <li>Use full batch instead of mini-batch [ðŸ®²ðŸ®³ Kunal]</li>
                    </ul>
                    <li>Architecture</li>
                    <ul>
                        <li>Check other weight initializations [ðŸ®²ðŸ®³ Paulius]</li>
                        <li>Change the optimizer from Adam to Muon (or others) [ðŸ®²ðŸ®³ Mikel]</li>
                        <li>Adam LR scheduler (remove?) [ðŸ®²ðŸ®³ Mahvash]</li>
                        <li>Use multiple heads (same backbone network) [ðŸ®²ðŸ®³ Seth]</li>
                    </ul>
                    <!-- <li>Policy Losses</li>
                    <ul>
                        <li>Entropy bonus (remove?) [ðŸ®²ðŸ®³ Mikel]</li>
                    </ul> -->
                    <li>Mujuco-specific</li>
                    <ul>
                        <!-- <li>Learn the covariance for the continuous actions predictor</li> -->
                        <!-- <li>Try to use the same backbone for actor and critic</li> -->
                        <li>Observation clipping (remove?) [ðŸ®²ðŸ®³ Kunal]</li>
                        <li>Reward scaling (replace with symlog) [ðŸ®²ðŸ®³ Kunal]</li>
                    </ul>
                </ul>
                We decided to use the PPO implementation from the <a href="https://github.com/vwxyzjn/cleanrl">CleanRL</a> GitHub repository, because it contains all the 37 tricks from the blog post.
                We chose the <code>HalfCheetah-v4</code> environment for preliminary runs, with 10 experiment replications (seeds).
            </p>
        </section>
        <section>
            <h2>Baseline</h2>
            <time datetime="2025-07-29">July 29</time>
            <p>
                We ran the baseline with the default hyperparameters from the CleanRL codebase:
                <iframe src="https://wandb.ai/occam-shaving-rl/baselines/reports/Copy-of-paulius-s-Baseline--VmlldzoxMzc4NzY1MQ" loading="lazy" style="border:none;height:786px;width:100%"></iframe>
                This set the bar we'd have to meet for our other experiments.
            </p>
        </section>
        <section>
            <h2>Individual Ablations</h2>
            <time datetime="2025-07-29">July 29</time>
            <p>
                We decided that we need to try out all of tricks removed, one by one.
                We call this first-order ablations.

                The results can be seen here:
                <iframe src="https://wandb.ai/occam-shaving-rl/baselines/reports/First-order-ablations--VmlldzoxMzgwMjQ0Ng" loading="lazy" style="border:none;height:1024px;width:100%"></iframe>
            </p>
            <p>
                Some of our conclusions:
                <ul>
                    <li>Muon is doing well.</li>
                    <li>Removing gradient clipping is doing well.</li>
                    <li>Chaning reward clipping to symlog is doing well.</li>
                    <li>Removing gradient normalization seeps to be doing well.</li>
                </ul>
            </p>
        </section>
        <section>
            <h2>Group Ablations</h2>
            <time datetime="2025-07-30">July 30</time>
            <p>
                We decided to try changing multiple things in a run, to see how they interact.
                The different combos we tried are:
                <ol>
                    <li>Combo 1</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>Smaller learning rate (0.0003)</li>
                        <li>Remove clipping</li>
                    </ul>
                    <li>Combo 2</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>Fullbatch</li>
                        <li>No learning rate scheduler</li>
                        <li>No normalization of the advantage function</li>
                        <li>No entropy bonus (KL term)</li>
                        <li>Remove observation clipping</li>
                        <li>Remove gradient clipping</li>
                    </ul>
                    <li>Combo 3</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>No gradient normalization</li>
                        <li>No learning rate scheudler</li>
                    </ul>
                    <li>Combo 4</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>GAE lambda = 1</li>
                        <li>No normalization of advantage function</li>
                        <li>No value function loss clipping</li>
                        <li>No entropy bonus (KL term)</li>
                        <li>No gradient normalization</li>
                        <li>Use shared (backbone) network</li>
                        <li>Smaller learning rate (0.00015)</li>
                    </ul>
                    <li>Combo 5</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>GAE lambda = 1</li>
                        <li>No normalization of advantage function</li>
                        <li>No value function loss clipping</li>
                        <li>No entropy bonus (KL term)</li>
                        <li>No gradient normalization</li>
                        <li>Use shared (backbone) network</li>
                        <li>Fullbatch</li>
                    </ul>
                    <li>Combo 6</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>GAE lambda = 1</li>
                        <li>No normalization of advantage function</li>
                        <li>No value function loss clipping</li>
                        <li>No entropy bonus (KL term)</li>
                        <li>No gradient normalization</li>
                        <li>Use shared (backbone) network</li>
                        <li>Fullbatch</li>
                        <li>Tuned learning rate (0.001)</li>
                    </ul>
                    <li>Combo 7</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>GAE lambda = 0</li>
                        <li>No normalization of advantage function</li>
                        <li>No value function loss clipping</li>
                        <li>No entropy bonus (KL term)</li>
                        <li>No gradient normalization</li>
                        <li>Use shared (backbone) network</li>
                        <li>Fullbatch</li>
                        <li>Tuned learning rate (0.001)</li>
                    </ul>
                    <li>Combo 8</li>
                    <ul>
                        <li>Muon optimizer</li>
                        <li>GAE lambda = 1</li>
                        <li>No normalization of advantage function</li>
                        <li>No value function loss clipping</li>
                        <li>No entropy bonus (KL term)</li>
                        <li>No gradient normalization</li>
                        <li>Use shared (backbone) network</li>
                        <li>Fullbatch</li>
                        <li>Tuned learning rate (0.001)</li>
                        <li>Change reward scaling to symlog</li>
                        <li>No LR scheduler</li>
                    </ul>
                </ol>
            </p>
        </section>
        <hr/>
        <section>
            <h2>References</h2>
            <ol>
                <li id="cite-37-details">Huang, Shengyi, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang. "The 37 implementation details of proximal policy optimization." The ICLR Blog Track 2023 (2022).</li>
                <li id="cite-cleanrl">Huang, Shengyi, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and JoÃ£o GM AraÃºjo. "Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms." Journal of Machine Learning Research 23, no. 274 (2022): 1-18.</li>
                <li id="cite-ppodetails">Andrychowicz, Marcin, Anton Raichuk, Piotr StaÅ„czyk, Manu Orsini, Sertan Girgin, RaphaÃ«l Marinier, Leonard Hussenot et al. "What matters for on-policy deep actor-critic methods? a large-scale study." In International conference on learning representations. 2021.</li>
            </ol>
        </section>
        <hr/>
        <section>
            <h2>Acknowledgements</h2>
            <ul>
                <li>Computational resources provided by Denvr Dataworks (sponsors of the DLRL 2025 Summer School at Amii) and Compute Canada (provided by students at the University of Alberta).</li>
                <li>Website design inspiration from <a href="http://bettermotherfuckingwebsite.com/">http://bettermotherfuckingwebsite.com/</a>.</li>
            </ul>
        </section>
    </main>
</body>
</html>